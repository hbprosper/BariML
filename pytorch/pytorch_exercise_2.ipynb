{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch - Classification - 2\n",
    " >__Created__: October 2019, Bari, Italy\n",
    " \n",
    "\n",
    "## Introduction\n",
    "In this example, we use a convolutional neural network (CNN) to classify the digits in the MNIST data set. Each image is represented by a $(28, 28)$ matrix, with matrix element values in the set $[0, 1/255, \\cdots, 1]$. In practice, a __batch__ of images is stored in a 4-index tensor $\\mathbf{x}_{ncij}$ of __shape__ $(N, C, H, W)$. Just think of this as a 4-dimensional array each cell of which contains a pixel value. The first index (dim = 0) labels the ordinal value of an image in a batch of $N$ images; the second index (dim = 1) labels the number of __channels__, $C$, which for a gray scale image is $C = 1$, and would be $C = 3$ for a red, green, blue (RGB) image, while the last two indices (dim = 2, 3) label the pixels of an image of height and width $H$ and $W$, respectively. \n",
    "\n",
    "### Model\n",
    "\n",
    "A typical convolutional neural network (CNN) comprises an alternating sequence of convolutional and coarse-graining (or __down-sampling__) layers ending with a fully connected feedforward network. A convolutional layer cross-correlates its inputs with a kernel, while the coarse-graining layer reduces the size of its inputs while (typically) increasing the number of output channels. Here is a high-level view of the model:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{y} & = \\mbox{softmax}(\\mathbf{b}_2 + \\mathbf{w}_2 {\\rm flatten}({\\rm relu} ({\\rm maxpool} (\\mathbf{b}_1 + {\\rm cc}(\\mathbf{w}_1, \\, {\\rm relu}({\\rm maxpool}(\\mathbf{b}_0 + {\\rm cc}(\\mathbf{w}_0, \\, \\mathbf{x})))))))),\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{b}$ and $\\mathbf{w}$, the biases and weights, are the parameters of the model. The functions are defined as follows:\n",
    "\n",
    "  * __softmax__ For $K$ output classes, \n",
    "\\begin{align*}\n",
    "(\\mbox{softmax}(x))_k & = \\frac{\\exp(x_{(k)})}{\\sum_{j=0}^{K-1} \\exp(x_{(j)})} ,\n",
    "\\end{align*}\n",
    "where $x_{(k)}$ denotes the $k^\\mbox{th}$ output of the previous layer. The\n",
    "softmax function\n",
    "is used to bound the output values to the unit interval and ensure that \n",
    "their sum is unity. \n",
    "\n",
    "  * __flatten__ Given an input tensor $x$, this function restructures $x$ into a 1-dimensional array.\n",
    "\n",
    "  * __relu__ Given an input tensor $x$, the function \n",
    "\\begin{align*}\n",
    "{\\rm relu}(x) &= \\begin{cases}\n",
    "    x, & \\text{if } x \\gt 0\\\\\n",
    "    0              & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "is applied\n",
    "*element-wise*, that is, to every element of the tensor.\n",
    "\n",
    "  * __maxpool__ Given an input tensor $x$, this function uses a moveable window to apply a coarse-graining of $x$. In effect, the moveable window  splits the tensor into non-overlapping pieces. Then, for each piece, the maximum value within that piece is returned. For example, if $x$ is a 2-d tensor, e.g., a matrix, of shape (28, 28), and the window is of shape (2, 2), maxpool splits the original matrix into non-overlapping pieces of shape (2,2) and returns the maximum value within each piece, thereby creating a new matrix of shape (14, 14).\n",
    "\n",
    "  * __cc__ Given two tensors $x$ and $y$, this function returns their cross-correlation.\n",
    "\n",
    "\n",
    "A high-level view is a good way to represent the structure of a model.  But, it is also good to try to understand the details...at least once. As is often true, a detailed mathematical description of a complicated function is made easier using an example. \n",
    "\n",
    "#### Convolutional layer\n",
    "In this example, the data into the first convolutional layer is a single channel (gray scale) image $\\mathbf{x}$ of shape $(N, 1, 28, 28)$, that is, a tensor comprising $N$ images, each with a single channel ($C = 1)$, and each a $(28, 28)$ matrix of pixels. This tensor is cross-correlated with a 4-d tensor of weights $\\mathbf{w}_0$, with shape $(10, 1, 5, 5)$. We shall refer to the latter as a __kernel__ and each of its $10 \\times 1$, $(5, 5)$, matrices (or 2-d tensors) as a __filter__. The first index of the kernel is the number of __output channels__ (10), the second is the number of __input channels__ (1). Therefore, a given $(5, 5)$ filter is identified by two indices: its input and output channel numbers. The last two indices of the kernel are the height (5) and width (5), respectively, of its filters. \n",
    "\n",
    "(Unfortunately, the jargon is not consistent across the machine learning literature. What we call a filter is indeed often referred to as a filter, but it is also called a kernel, and what we call a kernel, that is, a stack of one or more filters, often has no specific name attached to it!) \n",
    "\n",
    "The operation of the convolutional layer for filters with odd-numbered height and width, e.g. $(5, 5)$, can be written as\n",
    "\n",
    "\\begin{align}\n",
    "    (\\mathbf{b}_0)_k + (\\mathbf{w}_0 \\otimes \\mathbf{x})_{nkij} &= (\\mathbf{b}_0)_k + \\sum_{c=0}^{C-1} \\sum_{r=-2}^2 \\sum_{s=-2}^2  \\mathbf{x}_{nc,\\, i+r,\\,j+s} \\, (\\mathbf{w}_0)_{kcrs}, \\quad i, j = 0,\\cdots, 27 ,\n",
    "\\end{align}\n",
    "\n",
    "where, we have chosen to label the central matrix element of a filter with the indices $(0, 0)$ and the other elements with indices that can be positive or negative integers. In the above expression, for a given pixel $i, j$, the filter labeled $k, c$, is cross-correlated with channel $c$ of input image $n$ and a sum over the cross-correlated input channels, that is, over the index $c$, is performed. \n",
    "\n",
    "However, for the above computations to work as shown, we need (in effect) to surround each image with extra strips of pixels. This operation is called __padding__. In the following, we choose to pad each image with 2 strips of pixels, with value zero. This padding makes it possible to center the $(5, 5)$ filters on every pixel, $i, j$, within the image. Had we used, for example, $(3, 5)$ filters, the appropriate padding shape would be $(1, 2)$, that is, 1 extra strip of pixels along the top and bottom of the image and 2 extra strips along the left and right sides of the image. But we are free to make other choices.\n",
    "\n",
    "Since the input image is a $(28, 28)$ matrix padded with 2 strips of pixels around the boundary, and each filter is a $(5, 5)$ matrix,\n",
    "the output of the convolution step will be a tensor of shape $(N, 10, 28, 28)$ provided that we use a __stride__ of 1, that is, if during the cross-correlation, we shift the filters a horizontal or vertical step of 1 pixel. Other choices are possible. By\n",
    "construction, the output of this convolutional layer is $N$, 10-channel images, of the same size as the original. \n",
    "\n",
    "#### Coarse-graining layer\n",
    "A convolutional layer is usually followed by a coarse-graining layer that reduces the number of pixels per image. In this example, the number of pixels in the image created by the convolutional layer is reduced by a factor 2 in both directions; that is, each of the 10 channels of the $(28, 28)$ image is coarse-grained, or down-sampled, into a $(14, 14)$ channel by replacing the corresponding group of pixels of shape $(2, 2)$ with a pixel whose value is set to the largest value among the group of 4 pixels.  This operation, called __max-pooling__, can be expressed as\n",
    "\n",
    "\\begin{align}\n",
    "    {\\rm maxpool}(O)_{nkij} &= \\max{ \\{O_{nk,\\, 2i + r,\\, 2j + s} \\}_{r,\\,s = 0, 1} } \\quad i, j = 0,\\cdots, 13,\n",
    "\\end{align}\n",
    "\n",
    "where $O$ is the output of a convolutional layer. Again, other choices are possible.\n",
    "\n",
    "After max-pooling, the output tensor has shape $(N, 10, 14, 14)$. A relu __activation function__ is applied to each element of this tensor in order to complete one sequence of operations: cross-correlation, down-sampling, and activation. \n",
    "\n",
    "A second convolutional layer follows. This time, however, its input must match the output of the previous max-pooling operation, which contains $N$ sequences of images, each with 10 channels, and each a $(14, \\, 14)$ matrix padded with a 2-pixel boundary. The kernel in this layer contains $16 \\times 10$, $(5, 5)$, filters, that is, it is of shape $(16, 10, 5, 5)$. Each of the 160, $(5, 5)$, filters is cross-correlated with the corresponding input channel. The cross-correlated input channels are then summed pixel by pixel to yield  a 16-channel output image from this layer. In other words, we arrive at an output tensor of size $(N, 16, 14, 14)$.\n",
    "\n",
    "Finally, a second max-pooling layer is applied, which transforms the 16-channel image of shape $(14, 14)$ to a 16-channel image of shape $(7, 7)$. The image is then flattened into a 1-d tensor (basically, a 1-d array) of size $16 \\times 7 \\times 7 = 784$, which are the inputs of the final linear layer with 10 outputs. Finally, a softmax function is applied.\n",
    "\n",
    "\n",
    "### Discussion\n",
    "The net effect of the sequence of nested layers, prior to flattening, is to transform a multi-channel input image into a multi-channel output image. The hope is that the output image captures the most relevant features of the input image thereby improving the accuracy of the subsequent classification. But, why use cross-correlation?\n",
    "\n",
    "The intuition behind the cross-correlation operation is that natural images tend to have similar features in different parts of the image. A filter that is sensitive to vertical features would tend to produce an output image that enhances vertical features, while suppressing the expression of horizontal features. It therefore seems plausible that if one could design filters that are sensitive to different image features, the set of filters could potentially transform an image into another in which the most relevant features are enhanced while the least relevant are suppressed. \n",
    "\n",
    "In the early days of image recognition systems, researchers tried to describe in software what they considered to be the most relevant features in images. This proved to be extremely difficult and success was limited. Today hand-coding is no longer necessary because the values of the parameters of the kernels can be determined automatically by minimizing an appropriate loss function. However, the mere fact that a large number of images is needed to fit these parameters is evidence that current machine learning methods and models, while spectacularly successful, are still far removed from human learning methods and models. \n",
    "\n",
    "A young child need be shown labeled objects, cat, dog, doll, truck, car, just a few times before being able to classify them correctly. The child can do this even when the objects are presented to her or him in orientations that differ from the ones used during the \"training phase\". This extraordinary ability suggests that the child's brain constructs a fault tolerant model of each object using only its most relevant features and is able to match rapidly the input image with the stored, or perhaps auto-reconstructed, fault tolerant model. The model is clearly fault tolerant because the child is able to ignore \"faults\" such as viewing the objects in lighting that differs from that used during the \"training phase\", or ignoring the fact that now the car has lost a couple of wheels or the doll is now wearing different clothes and so on. \n",
    "\n",
    "In spite of the truly impressive recent successes, it is clear that machine learning-based artificial intelligence still has a long way to go. Of course, it could be that the age-old goal of building machines that think like humans is simply the wrong goal. After all, there are already many instances in which researchers have inadvertently succeeded in producing machines that mimic our prejudices, which is hardly progress. Therefore, perhaps we should embrace the \"artificial\" in artificial intelligence and the possibility that these machines will inevitably \"think\" differently from us. The trick, of course, is to have them \"think\" in a way that improves the human condition. It would not be helpful if, for example, our super-intelligent machine concluded that the best way to solve climate change is to eliminate one of its culprits, namely, the species Homo sapiens!\n",
    "\n",
    "### Loss function\n",
    "\n",
    "See description in pytorch\\_exercise\\_1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import imageio as im\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#  a function to save results\n",
    "import joblib as jb\n",
    "\n",
    "# to reload modules\n",
    "import importlib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update fonts\n",
    "FONTSIZE = 14\n",
    "font = {'family' : 'sans-serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : FONTSIZE}\n",
    "mp.rc('font', **font)\n",
    "seed = 128\n",
    "rnd  = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Images\n",
    "If the files are do not exist, go to the __datasets__ folder and run the notebook __prepare_mnist_data.ipynb__ to create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = jb.load('../datasets/mnist_train.pkl')\n",
    "test_x,  test_y  = jb.load('../datasets/mnist_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot first few  images\n",
    "Use imshow(..) and show() to display the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImages(x, n_rows=2, n_cols=5, f_size=(10, 4)):\n",
    "    f, ax = plt.subplots(nrows=n_rows, \n",
    "                         ncols=n_cols, \n",
    "                         figsize=f_size)\n",
    "    # note use of flatten() to convert a matrix of shape (nrows, ncols)\n",
    "    # to a 1-d array.   \n",
    "    for image, ax in zip(x, ax.flatten()):\n",
    "        ax.imshow(image.reshape(28, 28), cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotImages(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter   = 10000            # number of iterations\n",
    "n_batch  = 128              # number of images/training batch\n",
    "learning_rate = 1.e-3\n",
    "train_fraction = 0.7        # fraction of data used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images for training:        42000\n",
      "number of images for validation:      18000\n",
      "validation batch size:                 5000\n",
      "number of images for testing:         10000\n"
     ]
    }
   ],
   "source": [
    "M = int(train_fraction*len(train_x))\n",
    "train_x, val_x = train_x[:M], train_x[M:]\n",
    "train_y, val_y = train_y[:M], train_y[M:]\n",
    "\n",
    "n_train = len(train_x)\n",
    "n_valid = len(val_x)\n",
    "n_test  = len(test_x)\n",
    "n_valid_b = 5000\n",
    "\n",
    "print(\"number of images for training:   %10d\" % n_train)\n",
    "print(\"number of images for validation: %10d\" % n_valid)\n",
    "print(\"validation batch size:           %10d\" % n_valid_b)\n",
    "print(\"number of images for testing:    %10d\" % n_test)\n",
    "\n",
    "x = torch.tensor(train_x[0].reshape(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model\n",
    "Use some magic to write the following cell to a file, which can then be imported into another notebook. But, it seems we need to import the saved\n",
    "class afterwards!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CNN.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile CNN.py\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        \"\"\"\n",
    "    A convolutional + fully-connected linear network.\n",
    "        \"\"\" \n",
    "        # call constructor of base (or super, or parent) class\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # Adding a padding of 2 will allow cross-correlation of the \n",
    "        # entire input image with the 10 filters. That way, we\n",
    "        # obtain output images that are the same size as the\n",
    "        # input images.\n",
    "        self.conv0 = nn.Conv2d(in_channels=1,    # input channels\n",
    "                               out_channels=10,  # output channels\n",
    "                               kernel_size=5,    # really filter size\n",
    "                               stride=1,       \n",
    "                               padding=2)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=10, \n",
    "                               out_channels=16, \n",
    "                               kernel_size=5,\n",
    "                               stride=1,\n",
    "                               padding=2)\n",
    "        \n",
    "        # we end with a linear layer. we need to compute the number\n",
    "        # of inputs to that layer. Ignoring the first index of the output\n",
    "        # tensor from conv1, which simply labels the ordinal value of \n",
    "        # the image in the batch of images, we note the following:\n",
    "        # 1. conv0 outputs a tensor of size (10, 28, 28).\n",
    "        #    we then coarse-grain to a tensor of size (10, 14, 14)\n",
    "        # 2. conv1 outputs a tensor of size (16, 14, 14), which is\n",
    "        #    coarse-grained to one of size (16, 7, 7)\n",
    "        # 3. therefore, when flattened, the number of inputs to the\n",
    "        #    linear layer is\n",
    "        #    16 * 7 * 7 = 784\n",
    "        self.n_inputs = 16 * 7 * 7\n",
    "        #    and we have 10 outputs, one for each digit\n",
    "        self.linear0 = nn.Linear(self.n_inputs, 10)\n",
    "        \n",
    "    def __str__(self):\n",
    "        s = ''\n",
    "        for layer in [self.conv0, \n",
    "                      self.conv1,\n",
    "                      self.linear0]:\n",
    "            s += '%s\\n' % layer\n",
    "        return s\n",
    "    \n",
    "    # define (required) method to compute output of network\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x:  input data (of type tensor) \n",
    "        \"\"\"\n",
    "        # conv0 expects a 4-d tensor, so reshape accordingly\n",
    "        # the -1 means the size is inferred from x\n",
    "        y = x.view(-1, 1, 28, 28)\n",
    "        \n",
    "        # 1. cross-correlate the input tensor of shape (-1, 1, 28, 28),\n",
    "        #    padded with a 2-pixel wide strip, with a (10, 1, 5, 5) kernel,\n",
    "        #    thereby producing an output tensor of shape (-1, 10, 28, 28)\n",
    "        # 2. coarse-grain with a (2,2) window, which shifts horizontally\n",
    "        #    and vertically 2 pixels at a time. This replaces the 10\n",
    "        #    (28, 28) channels by 10 (14, 14) channels, by replacing a \n",
    "        #    group of 4 pixels with one. The output tensor at this stage \n",
    "        #    has shape (-1, 10, 14, 14).\n",
    "        # 3. apply a relu function to every element of this tensor\n",
    "        y = F.relu(F.max_pool2d(self.conv0(y), 2))\n",
    "        \n",
    "        # 1. cross-correlate a (-1, 10, 14, 14) tensor, padded as above, \n",
    "        #    with a (16, 10, 5, 5) kernel and, for each (5, 5) filter, sum \n",
    "        #    over the 10 input channels. Since the kernel contains 16 \n",
    "        #    output channels, the end result is a 16-channel image. The \n",
    "        #    output therefore has shape (-1, 16, 14, 14).\n",
    "        # 2. coarse-grain with a (2,2) window, as above, thereby creating\n",
    "        #    an output tensor of shape (-1, 16, 7, 7).\n",
    "        # 3. apply a relu function element-wise (as above).\n",
    "        y = F.relu(F.max_pool2d(self.conv1(y), 2))\n",
    "       \n",
    "        # flatten the tensor (-1, 16, 7, 7) to the tensor (-1, 16*7*7).\n",
    "        y = y.view(-1, self.n_inputs)\n",
    "        \n",
    "        # During training (only), randomly dropout, that is, zero, \n",
    "        # half of the elements in the current tensor y. dropout has\n",
    "        # been shown to improve the training and yield better results.\n",
    "        # Dropout effectively deactivates all the weights attached \n",
    "        # to the zeroed element. Alternatively, it can be thought of as a\n",
    "        # way to apply random modifications to a multi-channel image by\n",
    "        # randomly setting half the pixels to zero at each iteration.\n",
    "        y = F.dropout(y, p=0.5, training=self.training)\n",
    "    \n",
    "        # Apply a linear transformation to the (-1, 784) tensor.\n",
    "        y = self.linear0(y)\n",
    "        \n",
    "        # Apply the softmax function horizontally, i.e., along \n",
    "        # the class axis (dim=1) in order to ensure that the outputs sum\n",
    "        # to unity.\n",
    "        # (Note: dim=0 is vertical, that is, along the batch axis.)\n",
    "        \n",
    "        # Final output: estimated class probabilities for ith image,\n",
    "        #   q_i(k) = exp(y_i(k) / sum_j exp(y_i(j)), j = 0,..., K-1,\n",
    "        # where K=10 is the number of classes and y_i(k) is the output \n",
    "        # for the ith feature vector for class index k. \n",
    "        \n",
    "        y = F.softmax(y, dim=1)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv0): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv1): Conv2d(10, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (linear0): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import CNN\n",
    "importlib.reload(CNN)\n",
    "model = CNN.Model()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageLoss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        outputs:  shape (batch_size, number_classes), pre-softmax outputs\n",
    "        targets:  shape (batch_size, ), class indices [0,...,C-1] \n",
    "        \"\"\"\n",
    "        batch_size = len(outputs)\n",
    "        \n",
    "        # ---------------------------------------------------------------       \n",
    "        # The cross entropy is defined by\n",
    "        #   H(p, q) = -sum_i p_i log(q_i)\n",
    "        #\n",
    "        # The entropy is defined by\n",
    "        #   H(p)    = -sum_i p_i log(p_i)\n",
    "        # and the Kullback-Leibler divergence by\n",
    "        #   D(p||q) = -sum_i p_i log(p_i/q_i)\n",
    "        #           = -sum_i p_i log(p_i) + sum_i p_i log(q_i)\n",
    "        #           = H(p) - H(p, q)\n",
    "        # Therefore, the cross entropy is minimized when the estimated \n",
    "        # probabilities q_i match the true probabilities p_i, in which \n",
    "        # case the cross entropy equals the entropy.\n",
    "        # --------------------------------------------------------------- \n",
    "        \n",
    "        # Note the numpy-like syntax for accessing elements of the\n",
    "        # tensor: outputs[list1, list2] \n",
    "        # Note also: range(batch_size) is [0,...batch_size-1]\n",
    "        #\n",
    "        # outputs is a (-1, 10) tensor. for each image pick the output \n",
    "        # value corresponding to its class. recall that targets is a 1-d \n",
    "        # tensor (basically, a 1-d array) of class labels. \n",
    "        outputs = outputs[range(batch_size), targets]\n",
    "        return -torch.sum(torch.log(outputs)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "avloss = AverageLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get batch\n",
    "Get a random sample from the training set of size $batch\\_size$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomBatch(train_x, train_y, batch_size):\n",
    "    rows    = rnd.choice(len(train_x), batch_size)\n",
    "    batch_x = train_x[rows]\n",
    "    batch_y = train_y[rows]\n",
    "    return (batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Fraction of correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, targets):\n",
    "    # For each image, return its predicted class label using argmax.\n",
    "    #\n",
    "    # argmax scans the numpy along the specified axis, here the \n",
    "    # horizontal axis, which is in the class direction, and returns the\n",
    "    # ordinal value of the maximum value, which is the predicted class. \n",
    "    # Note: outputs must be converted from a tensor to a numpy array \n",
    "    # before being passed to argmax.\n",
    "    outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "    # count how often the predicted class matches the actual class and\n",
    "    # compute the fraction of correct predictions. \n",
    "    # Note: targets must be converted to a numpy array for the comparison\n",
    "    # to work since outputs is a numpy array.\n",
    "    return float(np.sum(outputs==targets.data.numpy())) / len(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_x, train_y, batch_size):\n",
    "    model.train() # training mode\n",
    "    \n",
    "    # get a random sequence of training data\n",
    "    batch_x, batch_y = randomBatch(train_x, train_y, batch_size)\n",
    "\n",
    "    # convert data from numpy arrays to tensors\n",
    "    with torch.no_grad():  # no need to compute gradients wrt. to x, y\n",
    "        x = torch.from_numpy(batch_x)\n",
    "        y = torch.from_numpy(batch_y)       \n",
    "    \n",
    "    # compute output of model\n",
    "    outputs = model(x)\n",
    "    \n",
    "    # compute loss functions, given the model outputs and the targets\n",
    "    loss    = loss_fn(outputs, y)        \n",
    "    optimizer.zero_grad()  # clear previous gradients\n",
    "    loss.backward()        # compute gradients\n",
    "    optimizer.step()       # move one step\n",
    "        \n",
    "def validate(model, train_x, train_y, val_x, val_y):\n",
    "    model.eval() # evaluation mode\n",
    "    with torch.no_grad():  # no need to compute gradients wrt. to x, y\n",
    "        \n",
    "        # to speed things up, use a random subset of n_valid_b images.\n",
    "        batch_x, batch_y = randomBatch(train_x, train_y, n_valid_b)\n",
    "        x = torch.from_numpy(batch_x)\n",
    "        y = torch.from_numpy(batch_y)       \n",
    "        o = model(x)\n",
    "        acc_t = accuracy(o, y)\n",
    "          \n",
    "        batch_x, batch_y = randomBatch(val_x, val_y, n_valid_b)  \n",
    "        x = torch.from_numpy(batch_x)\n",
    "        y = torch.from_numpy(batch_y)      \n",
    "        o = model(x)\n",
    "        acc_v = accuracy(o, y)\n",
    "\n",
    "    return (acc_t, acc_v)\n",
    "               \n",
    "def trainModel(model, optimizer, loss_fn, \n",
    "               train_x, train_y,\n",
    "               val_x, val_y,\n",
    "               n_iterations, batch_size):\n",
    "    xx   = []\n",
    "    yy_t = []\n",
    "    yy_v = []\n",
    "\n",
    "    step = 100\n",
    "    \n",
    "    for ii in range(n_iterations):\n",
    "        train(model, optimizer, loss_fn, \n",
    "              train_x, train_y, \n",
    "              batch_size)\n",
    "\n",
    "        if ii % step == 0:\n",
    "            acc_t, acc_v = validate(model, \n",
    "                                    train_x, train_y, \n",
    "                                    val_x, val_y)\n",
    "            \n",
    "            print(\"\\r%10d\\t%10.4f\\t%10.4f\" % (ii, acc_t, acc_v), end='')\n",
    "        \n",
    "            xx.append(ii)\n",
    "            yy_t.append(acc_t)\n",
    "            yy_v.append(acc_v)\n",
    "            \n",
    "    return (xx, yy_t, yy_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose minimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0\t    0.1254\t    0.1300\n",
      "         1\t    0.1336\t    0.1364\n",
      "         2\t    0.1354\t    0.1290\n",
      "         3\t    0.1414\t    0.1424\n",
      "         4\t    0.1660\t    0.1630\n",
      "         5\t    0.2122\t    0.2058\n",
      "         6\t    0.3018\t    0.2936\n",
      "         7\t    0.4418\t    0.4430\n",
      "         8\t    0.5244\t    0.5300\n",
      "         9\t    0.5680\t    0.5680\n",
      "        10\t    0.5640\t    0.5760\n",
      "        11\t    0.5554\t    0.5856\n",
      "        12\t    0.5526\t    0.5782\n",
      "        13\t    0.5510\t    0.5704\n",
      "        14\t    0.5388\t    0.5584\n",
      "        15\t    0.5502\t    0.5558\n",
      "        16\t    0.5750\t    0.5910\n",
      "        17\t    0.6010\t    0.6260\n",
      "        18\t    0.5980\t    0.6042\n",
      "        19\t    0.6398\t    0.6568\n",
      "        20\t    0.6798\t    0.7006\n",
      "        30\t    0.7160\t    0.7270\n",
      "        40\t    0.7704\t    0.7804\n",
      "        50\t    0.8104\t    0.8214\n",
      "        60\t    0.8516\t    0.8566\n",
      "        70\t    0.8684\t    0.8696\n",
      "        80\t    0.8834\t    0.8816\n",
      "        90\t    0.8818\t    0.8890\n",
      "       100\t    0.8886\t    0.8864\n",
      "       200\t    0.9438\t    0.9400\n",
      "       300\t    0.9528\t    0.9520\n",
      "       400\t    0.9632\t    0.9526\n",
      "       500\t    0.9636\t    0.9628\n",
      "       600\t    0.9700\t    0.9674\n",
      "       700\t    0.9686\t    0.9656\n",
      "       800\t    0.9698\t    0.9756\n",
      "       900\t    0.9752\t    0.9736\n",
      "      1000\t    0.9756\t    0.9744\n",
      "      1500\t    0.9824\t    0.9780\n",
      "      2000\t    0.9854\t    0.9782\n",
      "      2500\t    0.9880\t    0.9842\n",
      "      3000\t    0.9872\t    0.9834\n",
      "      3500\t    0.9880\t    0.9848\n",
      "      4000\t    0.9888\t    0.9864\n",
      "      4500\t    0.9872\t    0.9868\n",
      "      5000\t    0.9894\t    0.9820\n",
      "      5500\t    0.9902\t    0.9868\n",
      "      6000\t    0.9900\t    0.9880\n",
      "      6500\t    0.9936\t    0.9876\n",
      "      7000\t    0.9924\t    0.9880\n",
      "      7500\t    0.9920\t    0.9884\n"
     ]
    }
   ],
   "source": [
    "xx, yy_t, yy_v = trainModel(model, optimizer, loss_fn,\n",
    "                            train_x, train_y, \n",
    "                            val_x,   val_y,\n",
    "                            n_iter,  n_batch)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(xx, yy_t, yy_v):\n",
    "    # Note: every element in a plot is an Artist object! \n",
    "    # create an empty figure\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    # add a subplot to it\n",
    "    nrows, ncols, index = 1,1,1\n",
    "    ax  = fig.add_subplot(nrows,ncols,index)\n",
    "    # adjust y limits\n",
    "    axes = ax.axes\n",
    "    axes.set_ylim((0.8, 1))\n",
    "    axes.set_xlim((0, xx[-1]))\n",
    "    \n",
    "    plt.plot(xx, yy_t, 'b', label='Training')\n",
    "    plt.plot(xx, yy_v, 'r', label='Validation')\n",
    "    plt.title('Training and Validation Errors')\n",
    "    plt.xlabel('Iterations', fontsize=16)\n",
    "    plt.ylabel('Accuracy', fontsize=16)\n",
    "    plt.grid(True, which=\"both\", linestyle='-')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1238931d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(xx, yy_t, yy_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model details\n",
      "  conv0.weight    \ttorch.Size([10, 1, 5, 5])\n",
      "  conv0.bias      \ttorch.Size([10])\n",
      "  conv1.weight    \ttorch.Size([16, 10, 5, 5])\n",
      "  conv1.bias      \ttorch.Size([16])\n",
      "  linear0.weight  \ttorch.Size([10, 784])\n",
      "  linear0.bias    \ttorch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(\"model details\")\n",
    "for key in model.state_dict():\n",
    "    print('  %-16s\\t%s' % (key, model.state_dict()[key].size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mnist_model_cnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
