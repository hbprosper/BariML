{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch - Classification - 1\n",
    " >__Created__: October 2019, Bari, Italy\n",
    " \n",
    "\n",
    "## Introduction\n",
    "We consider the benchmark task of classifying the $28 \\times 28 = 784$ gray scale pixel images of handwritten digits in the [MNIST](http://yann.lecun.com/exdb/mnist) data set using __supervised learning__. In supervised learning a model is trained (that is, fitted) to data consisting of unambiguously labeled __feature vectors__, here images. For classification, the label is the class to which a vector belongs, while for regression the labels are the values associated with the vectors. In the MNIST data set there are $K = 10$ classes, labeled by the digits 0 to 9. The data contains $T = 60,000$ images for training and 10,000 images for testing. Following good practice, we split the training data into a set for training and another for validating the training. The test data are not used in the training.\n",
    "\n",
    "The value of each pixel in the MNIST data set lies in the discrete set $[0, 1, \\cdots, 255]$. Again, following good practice, we rescale each datum so that it lies in the unit interval. In this exercise, the data lie in the (rational) set $D = [0, 1/255, \\cdots, 1]$. The MNIST training data may be viewed as a sample from an unknown probability distribution $p(t, x)$ defined on the finite discrete space $[0,\\cdots,9] \\times D^{784}$, where the __target__, $t$, is the class label associated with image $x$. ($p(t, x)$ is clearly an abstraction since it is the distribution of an *infinite* MNIST data set that exists only in the same sense as the set of real numbers.) An image is then a point $x \\in D^{784}$. If we associate each class with a different color, the training data form a swarm of colored points where points of a given color tend to \"flock\" together in the space $D^{784}$.\n",
    "\n",
    "In this exercise, a fully connected shallow neural network (SNN) is used to classify the MNIST digits. The training is done with __batches__ of images represented by a 2-index tensor $\\mathbf{x}_{nj}$ of __shape__ $(N, H \\times W)$, i.e., a matrix. The first index (along dimension dim = 0) labels the ordinal value $n$ of an image in a batch of $N$ images, while the second index (along dim = 1) labels the pixels of a *flattened* image of height and width $H$ and $W$, respectively.  \n",
    "\n",
    "### Model\n",
    "Machine learning models often have an aura of mystery about them. But, a sensible perspective can be maintained by remembering that regardless of how their form may have been motivated, these models are, ultimately, just exceedingly complicated non-linear functions. \n",
    "\n",
    "Another point should be noted before we delve into details. Much has been made of the sometimes spectacular failure in the application of machine learning to real-world problems. For example, suppose that a single pixel in an image is changed, say from the value 197/255 to the value 0. This causes the point representing the image in the space $D^{784}$ to suffer a displacement in that space. To the human brain, a single pixel change in a $28 \\times 28$ image is a negligible distortion that does not impair its ability to classify the image, either because the distortion goes unnoticed or it is ignored. But, to a machine learned classifier, a single pixel change could result in a large displacement in the abstract space of images, thereby making the image an outlier whose position is no longer representative of the distribution of *any* of the classes defined by the probability $p(t, x)$. The point is that the classifier depends on the probability $p(t, x)$, therefore, if the latter fails to capture the kinds of outliers one expects in real-world data, it ought not to surprise that the classifier may fail to classify an outlier correctly. \n",
    "\n",
    "One might think that the solution is to train with data that contains outliers. However, it may not be feasible to build a model that captures every sort of potential outlier. Moreover, if outliers in the training data are relatively rare, the training procedure may simply ignore them. A more practical approach might be to devise a way to recognize when an image is an outlier, detect which subsets of pixels are responsible for this and assign to the offending pixels values that are representative of their neighborhood. Having tamed the image in this way, it is then presented to the classifier.\n",
    "\n",
    "In this exercise, we use the following model \n",
    "\n",
    "$$\\mathbf{y} = \\mbox{softmax}(\\,\\mathbf{b}_1 + \\mathbf{w}_1 \\, \\mbox{relu}(\\mathbf{b}_0 + \\mathbf{w}_0 \\, \\mathbf{x}) \\, ),$$\n",
    "\n",
    "where $\\mathbf{b}$ and $\\mathbf{w}$, the biases and weights, are the parameters of the model and $\\mbox{relu}(x)$, which is a function applied to every element $x$ of its tensor argument (i.e., applied element-wise), is defined by\n",
    "\n",
    "\\begin{align*}\n",
    "\\mbox{relu}(x) & = \\begin{cases}\n",
    "    x, & \\text{if } x \\gt 0\\\\\n",
    "    0              & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "The parameters with suffix 0 pertain to the input layer, while those with suffix 1 pertain to the so-called __hidden__ layer.\n",
    "Since $\\mathbf{y}$ models class probabilities that sum to unity, it makes sense to map each output of the hidden layer to the unit interval by applying, for example, the softmax function for $K$ output classes, \n",
    "\n",
    "\\begin{align*}\n",
    "(\\mbox{softmax}(x))_k & = \\frac{\\exp(x_{(k)})}{\\sum_{j=0}^{K-1} \\exp(x_{(j)})} ,\n",
    "\\end{align*}\n",
    "\n",
    "where $x_{(k)}$ denotes the $k^\\mbox{th}$ output of the hidden layer. \n",
    "\n",
    "### Loss function\n",
    "Typically, a machine learning model is fitted to the training data by minimizing a suitably defined function, which in the statistics literature is often referred to as the __empirical risk__. (Fitting models to data is called learning by machine learning enthusiasts.) The empirical risk is a Monte Carlo approximation of the __risk functional__, defined by \n",
    "\n",
    "\\begin{align*}\n",
    "    R[f] &= \\int L(t, \\, f(x, \\theta)) \\, \n",
    "    p(t, \\, x) \\, dt \\, dx,\n",
    "\\end{align*}\n",
    "\n",
    "where $L(t, \\, f)$ is called the __loss function__ and measures how much one loses if the output of the parameterized function $f(x, \\theta)$ differs from the __target__ $t$. $\\color{blue}{\\rm Warning}$: In the machine learning world, the empirical risk is typicaly referred to as the loss function, when what is really meant is the average loss function. \n",
    "\n",
    "__Important Note__: In order for the risk functional $R[f]$ to reach its minimum, defined by variations of $f$ that yield the condition $\\delta R = 0$ for all $x$, the function $f$ must be sufficiently flexible. If the latter condition is satisfied, then the mathematical quantity approximated by $f$ depends solely on the form of the loss function $L(t, \\, f)$ and the probability distribution $p(t, \\, x)$ of the training data. In particular, it does not depend on\n",
    "    the details of the functon $f$ apart from its presumed flexibility. Of course, in practice, we do not minimize $R[f]$, but rather the empirical risk, which approximates it. Nevertheless, to the degree that a very large data set approximates an infinite one and to the degree that our minimizer is able to find a good approximation to the minimum, this bit of reasoning suggests that it is as least as important to think about the form of the loss function $L(t, \\, f)$ as it is to think about the form of the model. If we have two models of equal functional flexibility then, *a priori*, for the same loss function the models will approximate the same quantity. \n",
    "\n",
    "In order to motivate our choice of loss function, we start with the __Kullback-Leibler__ (KL) divergence\n",
    "\n",
    "\\begin{align*}\n",
    "    D(p || q) & = \\sum_i p_i \\log(p_i / q_i),\n",
    "\\end{align*}\n",
    "\n",
    "which is a *global* measure of the dissimilarity between a probability distribution $p$ and a reference distribution $q$. The KL divergence is a global measure in that it depends on the entire probability distributions $p$ and $q$. In particular, the measure does not supply information about where $p$ and $q$ are well matched and where they are not. \n",
    "\n",
    "The KL divergence is zero if and only if the two distributions are identical. Typically, we have a model for $q$, but $p$ is usually unknown, which makes it difficult to approximate $D(p || q)$. However, the KL divergence can be written as,\n",
    "\n",
    "\\begin{align*}\n",
    "  D(p || q) & = \\sum_i p_i \\log(p_i / q_i),\\\\\n",
    "            & = -\\sum_i p_i \\log q_i - \\left(-\\sum_i p_i \\log p_i\\right),\\\\\n",
    "            & = H(p, q) - H(p).\n",
    "\\end{align*}\n",
    "\n",
    "The quantity $H(p) = -\\sum_i p_i \\log p_i$ is called the __entropy__, while $H(p, q)$ is called the __cross entropy__. Like the KL divergence, the cross entropy is minimized when $q = p$, whereupon it equals the entropy of the probability distribution $p$. The practical advantage of the cross entropy over the KL divergence is that for training data distributed according to $p$, we can use the following Monte Carlo approximation of the cross entropy,\n",
    "\n",
    "\\begin{align*}\n",
    "  H(p, q) & = -\\sum_i p_i \\log q_i,\\\\\n",
    "            & \\approx -\\frac{1}{T}\\sum_{i=0}^{T-1} \\log q_i,\n",
    "\\end{align*}\n",
    "\n",
    "where $T$ is the number of feature vectors, here images, in the training set. In this exercise, we shall use cross entropy as the loss function.\n",
    "The outputs of the model are the conditional class probabilities\n",
    "\n",
    "\\begin{align*}\n",
    "    P(k \\, | \\, \\mathbf{x} ) & = \\frac{P(\\mathbf{x} \\, | \\, k) \\, \\pi(k)}{\\sum_{j=0}^K P(\\mathbf{x} \\, | \\, j) \\, \\pi(j)},\n",
    "\\end{align*}\n",
    "\n",
    "where $\\pi(k)$ is the prior probability of class $k$ and $P(\\mathbf{x} \\, | \\, k)$ is the probability of image $\\mathbf{x}$ *given* that it is of class $k$. ($P(\\mathbf{x} \\, | \\, k)$ is often referred to as the __likelihood__.) To compute the loss function, we set $q_i = P(k_i \\, | \\, \\mathbf{x}_i )$, where $k_i$ is the true class index of image $x_i$. In other words, we take $q_i$ to be the estimated conditional probability that image $x_i$ is of class $k_i$ given that it is known to be of that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# the standard module for tabular data\n",
    "import pandas as pd\n",
    "\n",
    "# the standard module for array manipulation\n",
    "import numpy as np\n",
    "\n",
    "# the standard modules for high-quality plots\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import imageio as im\n",
    "\n",
    "#  a function to save results\n",
    "import joblib as jb\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "\n",
    "# pytorch's automatic differentiation \n",
    "from torch.autograd import Variable\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update fonts\n",
    "FONTSIZE = 14\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : FONTSIZE}\n",
    "mp.rc('font', **font)\n",
    "\n",
    "# set a seed to ensure reproducibility\n",
    "seed = 128\n",
    "rnd  = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Images\n",
    "If the files are do not exist, go to the __datasets__ folder and run the notebook __prepare_mnist_data.ipynb__ to create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = jb.load('../datasets/mnist_train.pkl')\n",
    "test_x,  test_y  = jb.load('../datasets/mnist_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot first few  images\n",
    "Use imshow(..) and show() to display the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImages(x, n_rows=2, n_cols=5, f_size=(10, 4)):\n",
    "    f, ax = plt.subplots(nrows=n_rows, \n",
    "                         ncols=n_cols, \n",
    "                         figsize=f_size)\n",
    "    # note use of flatten() to convert a matrix of shape (nrows, ncols)\n",
    "    # to a 1-d array.\n",
    "    for image, ax in zip(x, ax.flatten()):\n",
    "        ax.imshow(image.reshape(28, 28), cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotImages(train_x)\n",
    "train_y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter   = 10000       # number of iterations\n",
    "n_batch  = 128         # N = n_batch\n",
    "learning_rate  = 1.e-3\n",
    "train_fraction = 0.7   # fraction of data used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images for training:        42000\n",
      "number of images for validation:      18000\n",
      "number of images for testing:         10000\n"
     ]
    }
   ],
   "source": [
    "M = int(train_fraction*len(train_x))\n",
    "train_x, val_x = train_x[:M], train_x[M:]\n",
    "train_y, val_y = train_y[:M], train_y[M:]\n",
    "\n",
    "n_train = len(train_x)\n",
    "n_valid = len(val_x)\n",
    "n_test  = len(test_x)\n",
    "\n",
    "print(\"number of images for training:   %10d\" % n_train)\n",
    "print(\"number of images for validation: %10d\" % n_valid)\n",
    "print(\"number of images for testing:    %10d\" % n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model\n",
    "\n",
    "Use some magic to write the following cell to the file __SNN.py__, which can then be imported into another notebook. But, it seems we need to import the saved\n",
    "class back into this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 8, 7])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SNN.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile SNN.py\n",
    "# A fully connected shallow neural network\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "    A fully-connected shallow neural network, that is, one with a\n",
    "    single hidden layer.\n",
    "        \"\"\" \n",
    "        # call constructor of base (or super, or parent) class\n",
    "        super(SNN, self).__init__()\n",
    "        \n",
    "        # INTRODUCTION\n",
    "        # linear model:   a model of the form y = b + w*x\n",
    "        # feature:        an attribute, e.g., a pixel value\n",
    "        # feature vector: an ordered list of features, e.g., an image\n",
    "        #\n",
    "        # The function nn.Linear(inputs, outputs)\n",
    "        # computes\n",
    "        #   y = b + w*x\n",
    "        # where x is a tensor of shape (number, inputs), w a tensor of\n",
    "        # shape (outputs, inputs), b a tensor of shape (outputs) and\n",
    "        # y is a tensor of shape (number, outputs).\n",
    "        #   number:  the batch size, that is, the number of feature \n",
    "        #            vectors used to compute the average loss, which, in\n",
    "        #            turn, is used to compute the local gradient in \n",
    "        #            a minimization step.\n",
    "        #   inputs:  the number of features/feature vector\n",
    "        #   outputs: the number of outputs of the linear model.\n",
    "        #\n",
    "        # DETAILS\n",
    "        # The first linear model has 784 inputs and 2500 outputs, \n",
    "        # followed by another linear model with 2500 inputs and \n",
    "        # 10 outputs. \n",
    "        #\n",
    "        # There are 784 * 2500 weights + 2500 biases in the first linear\n",
    "        # model. The linear model (the second layer) has 2500 * 10 \n",
    "        # weights + 10 biases. Therefore, the total number of parameters \n",
    "        # in this network is \n",
    "        # 784 * 2500 + 2500 + 2500 * 10 + 10 = 1,987,510, that is, about\n",
    "        # 2 million!\n",
    "        self.layer0 = nn.Linear(784, 2500)\n",
    "        self.layer1 = nn.Linear(2500,  10) # the \"hidden\" layer.\n",
    "        \n",
    "    def __str__(self):\n",
    "        s = ''\n",
    "        for layer in [self.layer0, \n",
    "                      self.layer1]:\n",
    "            s += '%s\\n' % layer\n",
    "        return s\n",
    "    \n",
    "    # this required method computes the output of the network\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x:  input data (of type Variable) \n",
    "        \"\"\"\n",
    "        # compute y = relu(b0 + w0*x) element-wise\n",
    "        y = F.relu(self.layer0(x))\n",
    "        \n",
    "        # during training, randomly dropout, that is, set to zero, \n",
    "        # half of the elements in the input tensor y. dropout has\n",
    "        # been shown to improve the training and yield better results.\n",
    "        y = F.dropout(y, p=0.5, training=self.training)\n",
    "               \n",
    "        # estimated class probabilities for ith feature vector, here an\n",
    "        # image,\n",
    "        #   q_i(k) = exp(y_i(k) / sum_j exp(y_i(j)), j = 0,..., K-1,\n",
    "        # where K=10 is the number of classes and y_i(k) is the output \n",
    "        # for the ith feature vector for class index k. \n",
    "        # apply the softmax function horizontally, i.e., along \n",
    "        # the class axis (dim=1)\n",
    "        # (dim=0 is vertical, that is, along the batch axis.)\n",
    " \n",
    "        y = F.softmax(self.layer1(y), dim=1)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of the model\n",
    "Note the import of the class that we've just saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SNN(\n",
       "  (layer0): Linear(in_features=784, out_features=2500, bias=True)\n",
       "  (layer1): Linear(in_features=2500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from SNN import SNN\n",
    "model = SNN()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define average loss (or empirical risk) function\n",
    "Here we use the cross-entropy,\n",
    "\\begin{align*}\n",
    "  H(p, q) & = -\\sum_i p_i \\log q_i,\\\\\n",
    "            & \\approx -\\frac{1}{T}\\sum_{i=0}^{T-1} \\log q_i.\n",
    "\\end{align*}\n",
    "\n",
    "In principle, we want $T \\rightarrow \\infty$, but, in practice, $T = batch\\_size$. This is done to produce a noisy approximation of\n",
    "the cross-entropy and therefore a noisy approximation of the local\n",
    "gradient. The stochasticity introduced increases the probability that the minimizer won't get stuck in a local minimum that is a poor approximation to the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageLoss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, outputs, targets):\n",
    "        \"\"\"\n",
    "    Compute the cross entropy defined by\n",
    "        H(p, q) = -sum_i p_i log(q_i),\n",
    "                = -(1 / T) sum_i=0^T-1 log(q_i)\n",
    " \n",
    "    which is minimized when the estimated probability q_i for the\n",
    "    ith feature vector matches the true probability p_i.\n",
    " \n",
    "    outputs: estimated class probabilities\n",
    "             shape: (batch_size, number_classes) \n",
    "    targets: class indices [0,...,K-1], shape: (batch_size, ) \n",
    "        \"\"\"\n",
    "        # For each feature vector i, pick the estimated probability q_i\n",
    "        # that corresponds to its true class index. The true class \n",
    "        # indices for the feature vectors (the images) are given in \n",
    "        # targets.\n",
    "        #\n",
    "        # Note the very powerful numpy-like syntax for accessing \n",
    "        # specific elements of an nd-array: outputs[list1, list2].\n",
    "        # basically, for each row i we pick the value in the column \n",
    "        # number given in target[i]. \n",
    "        # Note also: range(batch_size) is [0,...batch_size-1].\n",
    "        batch_size = len(outputs)\n",
    "        outputs = outputs[range(batch_size), targets]\n",
    "        \n",
    "        # compute H(p, q) = -sum_i p_i log(q_i)\n",
    "        return -torch.sum(torch.log(outputs)) / batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "avloss = AverageLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get batch\n",
    "Get a random sample from the training set of size $n\\_batch$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomBatch(train_x, train_y, batch_size):\n",
    "    # the numpy function choice(length, number)\n",
    "    # selects at random \"number\" integers from the range [0, length-1]\n",
    "    rows    = rnd.choice(len(train_x), batch_size)\n",
    "    batch_x = train_x[rows]\n",
    "    batch_y = train_y[rows]\n",
    "    return (batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Compute fraction of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, targets):\n",
    "    # We take the ordinal value of the highest of the 10 outputs as the\n",
    "    # predicted class index. Use the numpy function argmax to do this.\n",
    "    #\n",
    "    # argmax scans the numpy array along the specified axis, here the \n",
    "    # horizontal axis (axis=1), which is in the class direction, and \n",
    "    # returns the ordinal value of the maximum value along that axis, \n",
    "    # which will be the predicted class index.\n",
    "    #\n",
    "    # Note the required use of data.numpy() to return the tensor outputs \n",
    "    # as a numpy array, which is the argument type expected by argmax.\n",
    "    outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "    \n",
    "    # Count how many times the predicted class index matches the true \n",
    "    # class index of the image and convert to a fraction.\n",
    "    return float(np.sum(outputs==targets.data.numpy())) / len(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_x, train_y, batch_size):\n",
    "    # set mode to training so that training specific operations such \n",
    "    # as dropout are enabled.\n",
    "    model.train()\n",
    "    \n",
    "    # Get a random sample (a batch) of images\n",
    "    batch_x, batch_y = randomBatch(train_x, train_y, batch_size)\n",
    "        \n",
    "    # Convert the numpy arrays batch_x and batch_y, the images and \n",
    "    # class indices, respectively, to tensor types and then to Variable\n",
    "    # types. The later is the expected type into models and the loss\n",
    "    # functions (really average loss or empirical risk functions).\n",
    "    #\n",
    "    # The type Variable is the magic that permits automatic \n",
    "    # differentiation with respect to its arguments. However, we do not\n",
    "    # need this feature for the images and the class indices, so we\n",
    "    # disable it.\n",
    "    with torch.no_grad(): # no need to compute gradients wrt. x and y\n",
    "        x = Variable(torch.from_numpy(batch_x))\n",
    "        y = Variable(torch.from_numpy(batch_y))       \n",
    "\n",
    "    # compute the output of the model for the batch of images x\n",
    "    outputs = model(x)\n",
    "    \n",
    "    # compute a noisy approximation to the average loss\n",
    "    loss    = avloss(outputs, y)\n",
    "    \n",
    "    # use automatic differentiation to compute a noisy approximation\n",
    "    # of local gradient\n",
    "    optimizer.zero_grad()  # clear previous gradients\n",
    "    loss.backward()        # compute gradients\n",
    "    \n",
    "    # Finally, advance one step in the direction of steepest descent,\n",
    "    # using the noisy local gradient. Consider a 2-million dimensional \n",
    "    # hyper-surface defined by the average loss in the limit of an \n",
    "    # infinite training sample. In principle, that is the hyper-surface \n",
    "    # we really wish we could explore and whose global minimum we would\n",
    "    # like to find. But, since this is impossible, we instead construct\n",
    "    # at any given point on this hyper-surface a noisy approximation \n",
    "    # to it using the loss function averaged over a small batch of images. \n",
    "    # Then, we approximate the local gradient using the noisy local\n",
    "    # approximation to the hyper-surface. Necessarily, the gradient \n",
    "    # will be a noisy approximation to the ideal, or true, local gradient. \n",
    "    #\n",
    "    # This may seem like an odd thing to do. Surely, it is better to \n",
    "    # use as large a training sample as possible in order to provide a \n",
    "    # more accurate approximation to the hyper-surface at any given\n",
    "    # point? However, that is actually a bad idea! Firstly, using the \n",
    "    # full training sample to define the average loss would drastically \n",
    "    # slow down the minimization. Secondly, the hyper-surface is likely\n",
    "    # to be highly corrugated with a huge number of local minima. \n",
    "    # Gradient descent in such a landscape runs the risk that a minimizer\n",
    "    # could fall into a local minimum and stay there. If the local\n",
    "    # minimum is close to the global mininum there is no issue. But it\n",
    "    # would be an issue if the local minimum is far from the global \n",
    "    # minimum. \n",
    "    #\n",
    "    # The use of noisy estimates of local gradients makes it possible for\n",
    "    # a minimizer, just by chance, to escape from a local minimum and \n",
    "    # therefore more likely to find a local minimum closer to the global \n",
    "    # one. This idea suggests that perhaps we should begin with very \n",
    "    # noisy gradients. Then, if we notice the minimizer bouncing around \n",
    "    # neither going uphill or downhill on the average, then we could \n",
    "    # surmise that it is actually bouncing around a minimum. Of course,\n",
    "    # we can never be certain that this is the global minimum, but, if it\n",
    "    # is, it would make sense to gradually increase the batch size until\n",
    "    # the minimizer settles down.\n",
    "\n",
    "    optimizer.step()       # move one step\n",
    "        \n",
    "def validate(model, train_x, train_y, val_x, val_y):\n",
    "    # make sure we set evaluation mode so that training specific\n",
    "    # operations such as dropout are disabled.\n",
    "    model.eval() # evaluation mode\n",
    "    \n",
    "    with torch.no_grad(): # no need to compute gradients wrt. x and y\n",
    "        # compute accuracy using training sample\n",
    "        x = Variable(torch.from_numpy(train_x))\n",
    "        y = Variable(torch.from_numpy(train_y))\n",
    "        o = model(x)\n",
    "        acc_t = accuracy(o, y)\n",
    "  \n",
    "        # compute accuracy using validation sample\n",
    "        x = Variable(torch.from_numpy(val_x))\n",
    "        y = Variable(torch.from_numpy(val_y))\n",
    "        o = model(x)\n",
    "        acc_v = accuracy(o, y)\n",
    "\n",
    "    return (acc_t, acc_v)\n",
    "               \n",
    "def trainModel(model, optimizer, loss_fn, \n",
    "               train_x, train_y,\n",
    "               val_x, val_y,\n",
    "               n_iterations, batch_size):\n",
    "    xx   = []\n",
    "    yy_t = []\n",
    "    yy_v = []\n",
    "\n",
    "    for ii in range(n_iterations):\n",
    "        # do one step of minimization\n",
    "        train(model, optimizer, aveloss, \n",
    "              train_x, train_y, \n",
    "              batch_size)\n",
    "\n",
    "        # print out accuracies on training and validation \n",
    "        # data sets\n",
    "        if   ii < 20:\n",
    "            step = 1\n",
    "        elif ii < 100:\n",
    "            step = 10\n",
    "        elif ii < 1000:\n",
    "            step = 100\n",
    "        else:\n",
    "            step = 500\n",
    "        \n",
    "        if ii % step == 0:\n",
    "            acc_t, acc_v = validate(model, \n",
    "                                    train_x, train_y, \n",
    "                                    val_x, val_y)\n",
    "            \n",
    "            print(\"%10d\\t%10.4f\\t%10.4f\" % (ii, acc_t, acc_v))\n",
    "        \n",
    "            xx.append(ii)\n",
    "            yy_t.append(acc_t)\n",
    "            yy_v.append(acc_v)\n",
    "            \n",
    "    return (xx, yy_t, yy_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose minimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0\t    0.2887\t    0.2892\n",
      "         1\t    0.4103\t    0.4166\n",
      "         2\t    0.4828\t    0.4909\n",
      "         3\t    0.5802\t    0.5929\n",
      "         4\t    0.6817\t    0.6942\n",
      "         5\t    0.7584\t    0.7750\n",
      "         6\t    0.7939\t    0.8064\n",
      "         7\t    0.7953\t    0.8074\n",
      "         8\t    0.7840\t    0.7971\n",
      "         9\t    0.7895\t    0.8017\n",
      "        10\t    0.8014\t    0.8147\n",
      "        11\t    0.8082\t    0.8187\n",
      "        12\t    0.8116\t    0.8195\n",
      "        13\t    0.8069\t    0.8141\n",
      "        14\t    0.8081\t    0.8157\n",
      "        15\t    0.8104\t    0.8173\n",
      "        16\t    0.8184\t    0.8251\n",
      "        17\t    0.8317\t    0.8386\n",
      "        18\t    0.8426\t    0.8512\n",
      "        19\t    0.8523\t    0.8585\n",
      "        20\t    0.8554\t    0.8616\n",
      "        30\t    0.8705\t    0.8736\n",
      "        40\t    0.8903\t    0.8918\n",
      "        50\t    0.8961\t    0.8948\n",
      "        60\t    0.9069\t    0.9073\n",
      "        70\t    0.9135\t    0.9124\n",
      "        80\t    0.9145\t    0.9149\n",
      "        90\t    0.9120\t    0.9125\n",
      "       100\t    0.9109\t    0.9090\n",
      "       200\t    0.9445\t    0.9407\n",
      "       300\t    0.9563\t    0.9508\n",
      "       400\t    0.9625\t    0.9517\n",
      "       500\t    0.9690\t    0.9581\n",
      "       600\t    0.9745\t    0.9638\n",
      "       700\t    0.9765\t    0.9649\n",
      "       800\t    0.9793\t    0.9666\n",
      "       900\t    0.9794\t    0.9667\n",
      "      1000\t    0.9810\t    0.9670\n",
      "      1500\t    0.9847\t    0.9691\n",
      "      2000\t    0.9893\t    0.9729\n",
      "      2500\t    0.9929\t    0.9759\n",
      "      3000\t    0.9925\t    0.9742\n",
      "      3500\t    0.9945\t    0.9754\n",
      "      4000\t    0.9949\t    0.9752\n",
      "      4500\t    0.9971\t    0.9781\n",
      "      5000\t    0.9974\t    0.9772\n",
      "      5500\t    0.9976\t    0.9777\n",
      "      6000\t    0.9979\t    0.9780\n",
      "      6500\t    0.9988\t    0.9795\n",
      "      7000\t    0.9978\t    0.9779\n",
      "      7500\t    0.9978\t    0.9788\n",
      "      8000\t    0.9985\t    0.9797\n",
      "      8500\t    0.9985\t    0.9788\n",
      "      9000\t    0.9989\t    0.9804\n",
      "      9500\t    0.9992\t    0.9801\n"
     ]
    }
   ],
   "source": [
    "xx, yy_t, yy_v = trainModel(model, optimizer, avloss,\n",
    "                            train_x, train_y, \n",
    "                            val_x, val_y,\n",
    "                            n_iter, n_batch)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(xx, yy_t, yy_v):\n",
    "    # Note: every element in a plot is an Artist object! \n",
    "    # create an empty figure\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    # add a subplot to it\n",
    "    nrows, ncols, index = 1,1,1\n",
    "    ax  = fig.add_subplot(nrows,ncols,index)\n",
    "    # adjust y limits\n",
    "    axes = ax.axes\n",
    "    axes.set_ylim((0.8, 1))\n",
    "    axes.set_xlim((0, xx[-1]))\n",
    "    \n",
    "    plt.plot(xx, yy_t, 'b', label='Training')\n",
    "    plt.plot(xx, yy_v, 'r', label='Validation')\n",
    "    plt.title('Training and Validation Errors')\n",
    "    plt.xlabel('Iterations', fontsize=16)\n",
    "    plt.ylabel('Accuracy', fontsize=16)\n",
    "    plt.grid(True, which=\"both\", linestyle='-')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(xx, yy_t, yy_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mnist_model_snn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
